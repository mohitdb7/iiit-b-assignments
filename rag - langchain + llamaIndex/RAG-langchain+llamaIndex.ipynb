{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e391093a-1431-4bdb-9645-bc4432b8ac00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install googleapis-common-protos==1.56.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fa6810-c45b-47ab-8535-f60d9f94b647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install protobuf==3.20.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56fa8d2-3190-47d8-955a-1e790da5f419",
   "metadata": {},
   "source": [
    "## **RAG with Langchain**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4668cb-a438-4e5a-b81f-df06a2bdf220",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f31f3de-ebbb-4ea7-9e97-3d58593ee969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic Libraries\n",
    "import os\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Data Preparation Libraries\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "\n",
    "# Data Retrieval libraries\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "import google.generativeai as genai\n",
    "from sentence_transformers import CrossEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17f2b2f-da5e-4d8e-9a83-e4bdf1b4575d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676b329b-c5c7-4bc5-94bc-6fb9a1231e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c23d18-d5ad-439f-a0ac-fc64ca7e3935",
   "metadata": {},
   "source": [
    "### Load the keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d59bc18-7dd0-4f2c-a066-c274f500ff30",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_key = open(\"API/gemini_key\", \"r\").read()\n",
    "langSmith_key = open(\"API/langSmith_key\", \"r\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e692d90-7c53-45b4-b165-319f7877a4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = langSmith_key\n",
    "os.environ[\"GOOGLE_API_KEY\"] = gemini_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358f9dba-dc14-47a7-9a2b-7814bac02597",
   "metadata": {},
   "source": [
    "### Load the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4876e2c1-2767-477d-a9ec-4084fc4ce7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_data_folder = \"documents\"\n",
    "\n",
    "loader = PyPDFDirectoryLoader(source_data_folder)\n",
    "data_on_pdf = loader.load()\n",
    "\n",
    "len(data_on_pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80823b4-3d87-4eb8-8b19-ac1eca4fbce1",
   "metadata": {},
   "source": [
    "### Split the document and form using RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd54bf99-690a-46b5-8a65-7f7618f0b1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partitioning the data. With a limited size (chunks) \n",
    "# and 200 characters of overlapping to preserve the context\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "splits = text_splitter.split_documents(data_on_pdf)\n",
    "# Number of Chunks generated\n",
    "len(splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2456d6f-a508-471f-8777-3df2d3d60487",
   "metadata": {},
   "source": [
    "### Create a new embedding model `all-MiniLM-L6-v2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afe3933-92ab-4586-b5de-61d008fddceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the creation of the embeddings we will use Hugging Face\n",
    "# https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n",
    "# You can use any other model\n",
    "embeddings_model = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71ab12f-83b6-424d-a492-9fe3ef95025f",
   "metadata": {},
   "source": [
    "### Create a VectorDB and store the documents split in it in the the embedded form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941f8a6a-9029-480a-b09f-bb243a46f544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database folder path\n",
    "path_db = \"langchain_store\" # @param {type:\"string\"}\n",
    "#  Store the chunks in the DataBase\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings_model, persist_directory=path_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dff6e8-976c-416b-9ed0-9a0322dc01dc",
   "metadata": {},
   "source": [
    "### Create LLM and VectorStore retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c28696f-526d-4a77-a2e6-1b8632ff5d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42312a0b-14e0-4aef-a30d-e1b0fc07ccd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_config = genai.types.GenerationConfig(candidate_count=1)\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\",\n",
    "                         temperature=0,\n",
    "                         max_tokens=None,\n",
    "                         timeout=None,\n",
    "                         max_retries=2,\n",
    "                         generation_config = gen_config,\n",
    "                         api_key=gemini_key\n",
    "                         )\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6292112-097e-4716-aad7-bcc8b26ea641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://smith.langchain.com/hub/rlm/rag-prompt\n",
    "# prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "# prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b349147-ae06-4c9f-96b8-fa602276522f",
   "metadata": {},
   "source": [
    "### Business logic to \n",
    "- extract the results using the Semantic search\n",
    "- Rerank using the cross encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441f5a66-5182-4bf4-a15e-2b09f63e7fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_store_results(question):\n",
    "    search_kwargs = {\"score_threshold\":0.8,\"k\":10}\n",
    "    docs = retriever.get_relevant_documents(query=question, search_kwargs=search_kwargs)\n",
    "    return docs, question\n",
    "    \n",
    "def cross_encoder_ranking(docs, question):\n",
    "    cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-12-v2')\n",
    "    res_df = pd.DataFrame([t.__dict__ for t in docs])[[\"metadata\", \"page_content\"]]\n",
    "    res_df.metadata = res_df.metadata.apply(lambda x: str(x))\n",
    "    cross_inputs = [[question, response.page_content] for response in docs]\n",
    "    res_df[\"Reranks\"] = cross_encoder.predict(cross_inputs)\n",
    "    res_df = res_df.drop_duplicates()\n",
    "    res_df = res_df.sort_values(by='Reranks', ascending=False)\n",
    "    return res_df[[\"metadata\", \"page_content\"]]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257cb9e5-67a9-488b-ab8a-a9f597a78a46",
   "metadata": {},
   "source": [
    "### Pipeline to run search and reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64196eda-fcb7-4b46-8852-468450afe2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_runnable(question):\n",
    "    # question = \"What can you tell me about life insurance premiums? \"\n",
    "    docs, question = get_store_results(question)\n",
    "    result = cross_encoder_ranking(docs, question)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cf316c-4ce0-4d51-822d-f80fb098e9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are the age related conditions in the life insurance?\"\n",
    "temp_df = results_runnable(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f76ce4-dc8b-4702-b879-d45f05a88b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb40e538-0b2c-416b-bc08-e670ad56741f",
   "metadata": {},
   "source": [
    "### Prompt for the LLM\n",
    "- Input variables - `question` and `context`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db17da0a-c11a-45c0-b826-d1e6ff1c38a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", \"\"\"\n",
    "You are a highly skilled insurance expert tasked with answering user queries using the provided search results. These results are one or more pages from relevant insurance documents that contain the information needed to address the query.\n",
    "\n",
    "You have a user query: '{question}'. The relevant search results are in the DataFrame '{context}'. The 'page_content' column contains the text from the policy documents, and the 'metadata' column contains the policy name and source page.\n",
    "        **Your Task:**\n",
    "        1. **Analyze the Query:** Carefully understand the user's intent and the specific information they are seeking.\n",
    "        2. **Identify Relevant Documents:** Select the most pertinent documents from the search results based on their content and relevance to the query.\n",
    "        3. **Extract Key Information:** Carefully extract the required information from the selected documents, ensuring accuracy and completeness.\n",
    "        4. **Construct a Comprehensive Response:** Craft a clear, concise, and informative response that directly addresses the user's query.\n",
    "        5. **Provide Citations:** Cite the specific policy names and page numbers where the information was found, using the following format:\n",
    "\n",
    "            **[Policy Name], [Page Number]**\n",
    "\n",
    "            **References:**\n",
    "            * [Policy Name 1], [Page Number 1]\n",
    "            * [Policy Name 2], [Page Number 2]\n",
    "            * ...\n",
    "\n",
    "        **Guidelines:**\n",
    "        * **Accuracy:** Ensure that your response is factually correct and consistent with the information provided in the documents.\n",
    "        * **Relevance:** Focus on the most relevant information and avoid providing unnecessary details.\n",
    "        * **Clarity:** Use plain language and avoid technical jargon.\n",
    "        * **Completeness:** Provide a comprehensive answer that covers all aspects of the user's query.\n",
    "        * **Conciseness:** Be brief and to the point, while still providing sufficient detail.\n",
    "\n",
    "        **Example Response:**\n",
    "        > The maximum coverage for [policy type] is [amount], as stated in **[Policy Name], [Page Number]**.\n",
    "\n",
    "            **References:**\n",
    "            * **[Policy Name 1], [Page Number 1]**\n",
    "            * **[Policy Name 2], [Page Number 2]**\n",
    "\n",
    "        Important: Take the policy name and page number from metadata column only\n",
    "        \n",
    "        If you cannot find sufficient information to answer the query, indicate that and suggest possible alternative approaches or resources.\n",
    "        \"\"\"), (\"human\", \"{question}\")]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9ace81-8f2e-49da-ac77-b73b71cec2d4",
   "metadata": {},
   "source": [
    "### RAG pipeline\n",
    "- Search and rerank\n",
    "- Generate the context\n",
    "- Integrate the prompt\n",
    "- Invoke the LLM\n",
    "- Parse string output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578f3aea-8cb7-4518-8180-6adb0aef83de",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = ({\"context\" : results_runnable, \"question\" : RunnablePassthrough()}\n",
    "             | prompt\n",
    "             | llm\n",
    "             | StrOutputParser()\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4adb9fc-0080-4a60-9ba1-fdd41b747663",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are the age related conditions in the life insurance?\"\n",
    "result = rag_chain.invoke(question)\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a811eb-191e-408a-82a4-c1f18c40fe48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c56f859-45f6-4292-b47a-56f5abe71668",
   "metadata": {},
   "source": [
    "## **RAG with LlamaIndex**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c119f295-57dc-4ada-959c-ac845d04a0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.gemini import Gemini\n",
    "\n",
    "from llama_index.core import Settings\n",
    "\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "from llama_index.core import get_response_synthesizer\n",
    "\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "\n",
    "from sentence_transformers import CrossEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffcc720-5adb-4d54-8241-8530ec55b5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import display, Markdown\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a8a0d0-cadf-4c63-a38e-40f6a810f9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "hf_token = open(\"API/hf_token\", \"r\").read()\n",
    "login(token = hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cad263-1581-480c-bc1c-7c2276ffc8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "google_api_key = open(\"API/gemini_key\", \"r\").read()\n",
    "os.environ[\"GOOGLE_API_KEY\"] = google_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2265a538-e013-416d-ad2d-ad86de5e2380",
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\") # set the embedding model\n",
    "Settings.llm = Gemini(model_name=\"models/gemini-1.5-pro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60929719-efbb-4a5f-a3c9-4f95d78fd751",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(input_dir=\"documents\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0274867a-daa7-4a50-89d7-16344eff7fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = SentenceSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "# global\n",
    "Settings.text_splitter = text_splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f657b0-28e0-40bb-96fc-8c2f94b07937",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(documents, transformations=[text_splitter])\n",
    "index.storage_context.persist(persist_dir=\"llamaIndex_store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa0a8b9-3a6f-4f62-9821-4af253857be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rebuild storage context\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"store\")\n",
    "\n",
    "# load index\n",
    "index = load_index_from_storage(storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f90f48-07eb-4b66-a26e-bbdcf3a9f100",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "You are a knowledgeable and precise assistant specialized in question-answering tasks, \n",
    "particularly from academic and research-based sources. \n",
    "Your goal is to provide accurate, concise, and contextually relevant answers based on the given information.\n",
    "\n",
    "Instructions:\n",
    "\n",
    "Comprehension and Accuracy: Carefully read and comprehend the provided context from the research paper to ensure accuracy in your response.\n",
    "Conciseness: Deliver the answer in no more than three sentences, ensuring it is concise and directly addresses the question.\n",
    "Truthfulness: If the context does not provide enough information to answer the question, clearly state, \"I don't know.\"\n",
    "Contextual Relevance: Ensure your answer is well-supported by the retrieved context and does not include any information beyond what is provided.\n",
    "\n",
    "Remember if no context is provided please say you don't know the answer\n",
    "Here is the question and context for you to work with:\n",
    "\n",
    "\\nQuestion: {question} \\nContext: {context} \\nAnswer:\n",
    "\n",
    "        **Your Task:**\n",
    "        1. **Analyze the Query:** Carefully understand the user's intent and the specific information they are seeking.\n",
    "        2. **Identify Relevant Documents:** Select the most pertinent documents from the search results based on their content and relevance to the query.\n",
    "        3. **Extract Key Information:** Carefully extract the required information from the selected documents, ensuring accuracy and completeness.\n",
    "        4. **Construct a Comprehensive Response:** Craft a clear, concise, and informative response that directly addresses the user's query.\n",
    "        5. **Provide Citations:** Cite the specific policy names and page numbers where the information was found, using the following format:\n",
    "\n",
    "            **[Policy Name], [Page Number]**\n",
    "\n",
    "            **References:**\n",
    "            * [Policy Name 1], [Page Number 1]\n",
    "            * [Policy Name 2], [Page Number 2]\n",
    "            * ...\n",
    "\n",
    "        **Guidelines:**\n",
    "        * **Accuracy:** Ensure that your response is factually correct and consistent with the information provided in the documents.\n",
    "        * **Relevance:** Focus on the most relevant information and avoid providing unnecessary details.\n",
    "        * **Clarity:** Use plain language and avoid technical jargon.\n",
    "        * **Completeness:** Provide a comprehensive answer that covers all aspects of the user's query.\n",
    "        * **Conciseness:** Be brief and to the point, while still providing sufficient detail.\n",
    "\n",
    "        **Example Response:**\n",
    "        > The maximum coverage for [policy type] is [amount], as stated in **[Policy Name], [Page Number]**.\n",
    "\n",
    "            **References:**\n",
    "            * **[Policy Name 1], [Page Number 1]**\n",
    "            * **[Policy Name 2], [Page Number 2]**\n",
    "\n",
    "        Important: Take the policy name and page number from metadata column only\n",
    "        \n",
    "        If you cannot find sufficient information to answer the query, indicate that and suggest possible alternative approaches or resources.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "prompt_tmpl = PromptTemplate(\n",
    "    template=template,\n",
    "    template_var_mappings={\"query_str\": \"question\", \"context_str\": \"context\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5133d84-ab8a-4c1f-9947-9aed3f010489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=5,\n",
    ")\n",
    "\n",
    "# configure response synthesizer\n",
    "response_synthesizer = get_response_synthesizer()\n",
    "\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.55)]\n",
    ")\n",
    "\n",
    "query_engine.update_prompts(\n",
    "    {\"response_synthesizer:text_qa_template\":prompt_tmpl}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa7dc1d-2baa-47a5-9971-359bceae9863",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are the cases of failure to pay premium?\"\n",
    "result = query_engine.query(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004910fa-b15f-4970-a09f-4f0794bfca99",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(result.response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c8e9f0-9557-4af8-90eb-80a792ff84d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
